{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut1XADEOCSoY",
        "outputId": "6b2ec14d-b145-4b5d-cd46-93aa1c426ed2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset link -  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
        "sms = pd.read_csv('SMSSpamCollection', sep='\\t', names=[\"label\", \"message\"])\n",
        "\n",
        "#Data cleaning and preprocessing\n",
        "wordnet=WordNetLemmatizer()\n",
        "corpus = []\n",
        "for i in range(len(sms)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', sms['message'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    review = [wordnet.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)"
      ],
      "metadata": {
        "id": "17zv8fF-Fhgb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=sms['message']\n",
        "\n",
        "y=pd.get_dummies(sms['label'])\n",
        "y=y.iloc[:,1].values"
      ],
      "metadata": {
        "id": "YypUlKezF1iD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cTAqW6xsBd-c"
      },
      "outputs": [],
      "source": [
        "# 80-20 Train Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "\n",
        "# Creating the Tf-idf\n",
        "cv=TfidfVectorizer()\n",
        "X = cv.fit(corpus)\n",
        "x_train = cv.transform(X_train).toarray()\n",
        "x_test  = cv.transform(X_test).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model using Naive bayes classifier\n",
        "\n",
        "model = MultinomialNB().fit(x_train, y_train)\n",
        "y_pred=model.predict(x_test)"
      ],
      "metadata": {
        "id": "vI8njExDGQH-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building confusion matrix for spam classifier\n",
        "\n",
        "confusion_m=confusion_matrix(y_test, y_pred)\n",
        "acc=accuracy_score( y_test, y_pred)\n",
        "\n",
        "print('Accuracy of test dataset = ',acc)\n",
        "print('\\n')\n",
        "\n",
        "print (classification_report(y_test, y_pred) )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3hZp7ppGSdF",
        "outputId": "a06e6df0-dc7a-4b22-f026-cf400e7d2325"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of test dataset =  0.9641255605381166\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98       955\n",
            "           1       1.00      0.75      0.86       160\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.98      0.88      0.92      1115\n",
            "weighted avg       0.97      0.96      0.96      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipe = Pipeline([('vectorizer', cv), ('multinomialNB', model)])\n",
        "pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6bdkBCaBrr0",
        "outputId": "34fa2a13-1455-440d-f60c-129c90d5752f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', TfidfVectorizer()),\n",
              "                ('multinomialNB', MultinomialNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('spamclassifier.pkl','wb') as f:\n",
        "    pickle.dump(pipe, f)"
      ],
      "metadata": {
        "id": "mECxY8wjH_Qu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Free entry in 2 a wkly comp to win FA Cup final.\"\n",
        "y = pipe.predict([text])"
      ],
      "metadata": {
        "id": "dUrYXzY7IWIm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ham' if y[0]==0 else 'spam')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmyI9gpTIaho",
        "outputId": "a2ab4e9c-c5f0-43db-bc34-c11e2efa0da4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n"
          ]
        }
      ]
    }
  ]
}